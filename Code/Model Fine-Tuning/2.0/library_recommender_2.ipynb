{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = r\"D:\\Sharif University of Tech\\Data\\Library Recommender\\Pypi data\\OriginalItems.json\"\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw one meaningfull representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['text'] = data_df['Summary'].str[0] + \" \" + data_df['Description'].str[0]\n",
    "data_df['text'] = data_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start embedding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "data_df['embedding'] = data_df['text'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find similar libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_libraries(query, data_df, top_n=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = []\n",
    "\n",
    "    for _, row in data_df.iterrows():\n",
    "        library_embedding = row['embedding']\n",
    "        similarity = cosine_similarity(query_embedding, library_embedding)\n",
    "        similarities.append((row['Package'][0], similarity[0][0]))\n",
    "\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I need a library for read and proccess data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_libraries = find_similar_libraries(query, data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nsepy-v1 0.1', 0.98204875),\n",
       " ('printListModule 1.0.0', 0.98187697),\n",
       " ('pointstorm 1.1.9', 0.98175424),\n",
       " ('wyjtools 0.0.1', 0.98125494),\n",
       " ('takeme-client-python 2.9', 0.98088366)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_row = data_df[data_df['Package'].apply(lambda x: 'pandas' in x[0])]\n",
    "pandas_description = pandas_row['Description'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pandas_multi ============ Simple loop for reading multiple csv files (matching a certain pattern) as a ``pandas.DataFrame``. I'm aware this need can be solved in even one line of Python, but loading multiple similar csv's is just something that should be as easy as loading one csv. If you don't want to add a new dependency to your project, google what ``os.listdir`` and ``glob`` can do for you. Installation can be done by typing:: pip install pandas_multi Usage of ``pandas_multi.read_csvs`` has been kept as similar as possible to ``pandas.read_csv``:: import pandas_multi # #   Note that dataframes only work if you give them the non-descriptive name df # df = pandas_multi.read_csvs('./20180728*.csv') # if you provide it with a path to a folder and nothing else, it will assume # everything in the folder is a comma-separated file df = pandas_multi.read_csvs('./data/') # if this is not the case, do this: df = pandas_multi.read_csvs('./data/*.csv') All options that are available to ``pandas.read_csv`` or ``pandas.concat`` can be passed into ``pandas_multi.read_csvs`` and will be redirected to the appropriate underlying functions. If you wish to maintain a trace back to the original data, you can run the function with the keyword argument ``filenames_as_keys=True``. Note that you should no longer use the keyword argument ``keys``. This will be ignored. Please note that the API for concatenation of Excel sheets is not yet stable. \""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is theoreticly good, but functionally we could do much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start using NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bf534717894ce8a1a298c37b4840c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dc1489a9834a1bb33aaa649892d957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f21b828a42c41c49b83c709d248f467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7713ae1cb148e6a647d2404620d88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2e5c3c755e40ff91b31c29db4068b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32a130fe3542f6b528d5a0c9957741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14f166e0a5541f69a3cc69fb4e19e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b20c8db4f64490b3484d496f2a1194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1779c69a00d449ff8ecaf56e5695b42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455f0b0ba8b643669bf029800e2e732a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7e95c7ce4044e681f4cefb0244e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a284fa566d354817acf18954711313c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ff5f00e16c419b9e724f6a3fd413d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60747d3755e04138aa8bc113a9b67567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return text.lower()\n",
    "\n",
    "def get_embedding(text):\n",
    "    processed_text = preprocess(text)\n",
    "    embedding = model.encode(processed_text, convert_to_tensor=True)\n",
    "    return embedding.numpy()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_libraries(query, data_df, top_n=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = []\n",
    "\n",
    "    for index, row in data_df.iterrows():\n",
    "        library_embedding = row['embedding']\n",
    "        similarity = cosine_similarity([query_embedding], [library_embedding])\n",
    "        similarities.append((row['Package'][0], similarity[0][0]))\n",
    "\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['embedding'] = data_df['text'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bankreport 0.1.6', 0.5945563), ('topsis-jatin-101703263 1.0', 0.5806109), ('only-common 2.1.0', 0.5434755), ('csvconvert 0.1', 0.53128356), ('bsdb 2.0.1', 0.5235471)]\n"
     ]
    }
   ],
   "source": [
    "query = \"csv\"\n",
    "similar_libraries = find_similar_libraries(query, data_df)\n",
    "print(similar_libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
