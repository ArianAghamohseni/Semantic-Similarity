{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "class SemanticSearchModel:\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "            SentenceTransformer('paraphrase-MiniLM-L6-v2'),\n",
    "            SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "        ]\n",
    "        self.hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "        self.faiss_indexes = {}\n",
    "        self.data_df = None\n",
    "        self.ensemble_embeddings = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def generate_ensemble_embeddings(self, texts):\n",
    "        all_embeddings = []\n",
    "        for model in self.models:\n",
    "            embeddings = model.encode(texts, show_progress_bar=False, convert_to_tensor=True)\n",
    "            all_embeddings.append(embeddings)\n",
    "        concatenated_embeddings = torch.cat(all_embeddings, dim=1)\n",
    "        return concatenated_embeddings.cpu().numpy()\n",
    "\n",
    "    def fit(self, data_df):\n",
    "        self.data_df = data_df\n",
    "        self.data_df['text'] = self.data_df['Summary'].str[0] + \" \" + self.data_df['Description'].str[0]\n",
    "        self.data_df['text'] = self.data_df['text'].fillna('')\n",
    "        self.data_df['text'] = self.data_df['text'].apply(self.preprocess_text)\n",
    "        self.ensemble_embeddings = self.generate_ensemble_embeddings(self.data_df['text'].tolist())\n",
    "        hdbscan_labels = self.hdbscan_clusterer.fit_predict(self.ensemble_embeddings)\n",
    "        self.data_df['hdbscan_cluster'] = hdbscan_labels\n",
    "        refined_labels = self.refine_clusters_with_kmeans(self.ensemble_embeddings, hdbscan_labels)\n",
    "        self.data_df['refined_cluster'] = refined_labels\n",
    "        self.build_faiss_indexes(refined_labels)\n",
    "\n",
    "    def refine_clusters_with_kmeans(self, embeddings, hdbscan_labels, n_subclusters=5):\n",
    "        unique_clusters = set(hdbscan_labels) - {-1}\n",
    "        refined_labels = np.array(hdbscan_labels)\n",
    "\n",
    "        for cluster_id in unique_clusters:\n",
    "            mask = hdbscan_labels == cluster_id\n",
    "            cluster_embeddings = embeddings[mask]\n",
    "\n",
    "            kmeans = KMeans(n_clusters=n_subclusters, random_state=42)\n",
    "            kmeans_labels = kmeans.fit_predict(cluster_embeddings)\n",
    "\n",
    "            refined_labels[mask] = kmeans_labels + cluster_id * n_subclusters\n",
    "\n",
    "        return refined_labels\n",
    "\n",
    "    def build_faiss_indexes(self, refined_labels):\n",
    "        dimension = self.ensemble_embeddings.shape[1]\n",
    "        for cluster_id in set(refined_labels):\n",
    "            cluster_mask = refined_labels == cluster_id\n",
    "            cluster_embeddings = self.ensemble_embeddings[cluster_mask].astype('float32')\n",
    "\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "            index.add(cluster_embeddings)\n",
    "            self.faiss_indexes[cluster_id] = index\n",
    "\n",
    "    def semantic_search_refined_cluster(self, query, top_n=5):\n",
    "        query_processed = self.preprocess_text(query)\n",
    "        query_embedding = np.hstack([\n",
    "            model.encode([query_processed], convert_to_tensor=True).cpu().numpy()\n",
    "            for model in self.models\n",
    "        ]).astype('float32')\n",
    "\n",
    "        refined_cluster_id = self.find_nearest_cluster(query_embedding)\n",
    "\n",
    "        if refined_cluster_id in self.faiss_indexes:\n",
    "            index = self.faiss_indexes[refined_cluster_id]\n",
    "            _, top_n_indices = index.search(query_embedding.reshape(1, -1), top_n)\n",
    "\n",
    "            cluster_mask = self.data_df['refined_cluster'] == refined_cluster_id\n",
    "            cluster_libraries = self.data_df[cluster_mask].iloc[top_n_indices[0]]\n",
    "            return cluster_libraries[['Package', 'Summary', 'Description']]\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def find_nearest_cluster(self, query_embedding):\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.ensemble_embeddings)\n",
    "        nearest_index = np.argmax(similarities)\n",
    "        return self.data_df.iloc[nearest_index]['refined_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file_path = r\"D:\\Sharif University of Tech\\Data\\Library Recommender\\Pypi data\\1\\Pypi_data_Feb_19_2024.json\"\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemanticSearchModel()\n",
    "model.fit(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, 'semantic_search_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_time = time.time() - start_time\n",
    "\n",
    "cpu_usage = psutil.cpu_percent()\n",
    "ram_usage = psutil.virtual_memory().percent\n",
    "\n",
    "with open('metrics.txt', 'w') as f:\n",
    "    f.write(f'CPU Usage: {cpu_usage}%\\n')\n",
    "    f.write(f'RAM Usage: {ram_usage}%\\n')\n",
    "    f.write(f'Execution Time: {execution_time} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I need a package for dataset management\"\n",
    "similar_libraries = model.semantic_search_refined_cluster(query, 10)\n",
    "print(similar_libraries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
